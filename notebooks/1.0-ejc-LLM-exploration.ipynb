{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM exploration - Alternative to OpenAI GPT API**\n",
    "\n",
    "This notebook explores the performance of other Hugging Face's LLMs as alternative to OpenAI's GPT APIs. The goal is to save cost by using open-source and free models while maintaining the accuracy of summarization task. In particular, the notebook explores the summarization of news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I. Sample Data**\n",
    "\n",
    "Hard-code a sample news data extracted by the original notebook of my project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = [\n",
    "    {\n",
    "        \"Title\": \"2nd Senate panel backs phaseout of Pogos\",\n",
    "        \"Content\": \"\\n The Senate committee on public order and dangerous drugs is supporting the proposed phaseout of Philippine offshore gaming operators (Pogos) but is leaving the decision to Malacañang, which can opt to retain them under more stringent regulations, Sen. Ronald dela Rosa said on Saturday.\\n In a radio interview, Dela Rosa, committee chair, said that while he agrees with the findings of the Senate committee on ways and means for the phaseout of Pogos, his panel is recommending a longer period of one to two years to allow Pogos to wind down their operations.\\n “I signed the [ways and means] committee report to allow it to be endorsed to the plenary for discussions, but I have manifested some reservation and that I will interpellate because what it recommended is outright banning,” he said.\\n Dela Rosa’s committee is the second of two panels that conducted a joint investigation of the Pogo industry to look into the economic benefits derived from it and weighed them against the crimes caused by its presence.\\n The senator said that while his committee has yet to release the committee report, it was inclined to support a gradual phaseout of Pogos, but for a period longer than the three months proposed by the other committee, chaired by Sen. Sherwin Gatchalian.\\n “I’m still for the phasing out of Pogo, but not outright. It must have a phasing out period,” he said in a radio interview.\\n According to Dela Rosa, he is recommending an outright ban on illegal Pogo firms, but the legitimate ones must be given a one to two year extension to give them time to “adjust” and find alternatives to their industry.\\n Dela Rosa said he would withdraw his signature to the Gatchalian panel’s report if it pushes an immediate ban on Pogos, saying it would be “unfair” to Pogo investors.\\n “For my part, I will not vote for outright banning because I am for a gradual phaseout to allow all stakeholders to adjust,” he said.\\n In the event Malacañang allows the continued operation of Pogos, Dela Rosa said his panel would propose that all its operations be done inside a “Pogo zone” for a more stringent law enforcement.\\n “All regulatory activities by the [Bureau of] Immigration, Pagcor (Philippine Gaming and Amusement Corp.) Philippine National Police and National Bureau of Investigation shall be undertaken within this Pogo zone,” he said.\\n On Tuesday evening, Gatchalian filed his committee report after months of delay.\\n The committee concluded that the social costs brought by Pogo operations outweighed the purported benefits from them in the form of tax payments and income from business rentals.\\n \",\n",
    "    },\n",
    "    {\n",
    "        \"Title\": \"AFP expects 2 million reserves per year from ROTC revival\",\n",
    "        \"Content\": \"\\n Around 2 million will be added every year to the reserve force of the Armed Forces of the Philippines once the law making the Reserve Officers’ Training Corps (ROTC) mandatory again is passed by Congress.\\n “Every year, if ROTC becomes mandatory, we expect an additional 2 million students from all universities,” Maj. Gen. Joel Alejandro Nacnac, deputy chief of staff for reservists and retiree affairs of the AFP, told a press conference in Camp Aguinaldo on Friday as the AFP observes the National Reservists Week..\\n According to Nacnac, the projected servicemen from the ROTC program will be classified as part of the “standby reserve” forces, who may be mobilized or ordered to active duty in times of national emergency or war.\\n They are different from the “ready reserve,” who may be called at any time to augment the regular armed force of the military “not only in times of war or national emergency but also to meet local emergencies arising from calamities, disasters and threats to peace, order, security and stability in any locality.”\\n As of June, the AFP reserve force was estimated at 1.2 million, composed of 71,000 ready reservists, 1.1 million standby reservists, and more than 15,000 affiliated units from other organizations.\\n Maj. Gen. Joseph Ferrous Cuison, head of the Philippine Naval Reserve Command, said they were employing the assistance of Philippine Navy affiliated reserve unit, composed of crews from fishing and shipping companies, in monitoring the West Philippine Sea.\\n “Right now, we cannot confront head-on the maritime militia of China, so what we do right now is just monitor. They report to us on what is happening there,” he said.\\n \",\n",
    "    },\n",
    "    {\n",
    "        \"Title\": \"Hackers attack PhilHealth’s website, systems\",\n",
    "        \"Content\": \"\\n MANILA, Philippines — Computer hackers attacked the website and online application of the Philippine Health Insurance Corp. (PhilHealth) on Friday, taking down the systems and blocking access for more than 24 hours.\\n In a statement on Saturday, PhilHealth said it has started containment measures as well as an investigation of the “information security incident,” together with the Department of Information and Communications Technology and other concerned government agencies “to assess its extent.”\\n “While investigation is being undertaken, affected systems shall be temporarily shut down to secure our application systems,” PhilHealth president and CEO Emmanuel Ledesma said.\\n The head of the state insurer appealed for understanding and assured that “we will get to the bottom of this and will institute stronger systems to prevent this from happening again.”\\n The Inquirer reached out to multiple officials and staff of PhilHealth to confirm whether the incident was a Medusa ransomware attack, but they have yet to reply.\\n A ransomware is a cyberattack that holds one entity’s data or system hostage until a ransom is paid.\\n In a February 2023 report by the US Department of Health and Human Services, MedusaLocker, a ransomware variant, was able to “infect and encrypt systems, primarily targeting the health-care sector, after it was first detected in September 2019.\\n MedusaLocker, deemed as “lesser known but potent,” leveraged the disorder and confusion during the COVID-19 pandemic to launch attacks, the report said.\\n According to a cybersecurity company last week, a firm usually spends about P55 million or about $1 million to resolve a single data breach and pay off ransom to regain system access.\\n \",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test using 1st sample\n",
    "title = news_data[0][\"Title\"]\n",
    "content = news_data[0][\"Content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **II. LLM Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 1**: Using GPT 3.5 API of OpenAI\n",
    "- **LIMITATION**: Not free and open-source\n",
    "    - This is the main reason why we bother to explore other LLM options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "\n",
    "# Enter API key from https://platform.openai.com/account/api-keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-###############\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an expert journalist and a reader that can highlight the most important points written in a news article; \n",
    "    You need to summarize a news article in just 1 sentence similar to a headline but more descriptive. \n",
    "    Add a second sentence only if necessary. Limit the total words between 20 to 30 words and do not use too many commas. Make it sound logical;\n",
    "\n",
    "    TITLE: {title}\n",
    "    CONTENT: {content}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"title\", \"content\"])\n",
    "\n",
    "summarizer_llm = LLMChain(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.5), prompt=prompt, verbose=True\n",
    ")\n",
    "\n",
    "summarized_news = summarizer_llm.predict(title=title, content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 2**: Using Llama2 in Langchain\n",
    "- Link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "    - **IMPORTANT NOTE**: Need to request access first to Meta before using this in HF. Link: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\n",
    "- **LIMITATION**: Computationally expensive to run at inference (resource-constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE FOR 1ST TIME USE OF HUGGING FACE IN PYTHON**: \n",
    "- For first time use of deployed models in Hugging Face, it will require you to insert a User Access Token\n",
    "    - Learn more: https://huggingface.co/docs/huggingface_hub/quick-start\n",
    "- At first run of a code with the ```model```, it will download the model in your local machine. To know the current file sizes, please check the \"Files and versions\" tab in the link of model.\n",
    "- Downloaded models were saved in cache file for Windows machines. Run the code below to locate its folder path\n",
    "    - Learn more: https://huggingface.co/docs/huggingface_hub/guides/manage-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login() # Enter token here from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Earl/.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "# Locates cache folder of HF where the models were downloaded\n",
    "from transformers import file_utils\n",
    "\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install -q langchain transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline, LLMChain, PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32,\n",
    "    #  load_in_4bit=True,\n",
    "    #  bnb_4bit_quant_type=\"nf4\",\n",
    "    #  bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    top_k=30,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT, citation=None):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "    if citation:\n",
    "        prompt_template += f\"\\n\\nCitation: {citation}\"  # Insert citation here\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "def generate(text, citation=None):\n",
    "    prompt = get_prompt(text, citation=citation)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, \"\")\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    wrapped_text = textwrap.fill(text, width=100)\n",
    "    print(wrapped_text + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an expert journalist that can highlight the most important points in a news article.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe, model_kwargs={\"temperature\": 0.7, \"max_length\": 256, \"top_k\": 50}\n",
    ")\n",
    "\n",
    "system_prompt = \"You are an expert journalist that can highlight the most important points in a news article.\"\n",
    "instruction = \"\"\"You need to summarize a news article in just 1 sentence similar to a headline but more descriptive. \n",
    "Add a second sentence only if necessary. Limit the total words between 20 to 30 words and do not use too many commas. \n",
    "Make it sound logical:\\n\\n CONTENT: {content}\"\"\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "summarizer_llm = LLMChain(prompt=prompt, llm=llm, verbose=False)\n",
    "\n",
    "summarized_news = summarizer_llm.run(content)\n",
    "print(summarized_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 2.2**: Using QUANTIZED Llama2 in Langchain\n",
    "- Link: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML\n",
    "- **LIMITATION**: No control on maximum number of context (token) length so it suffers a lot on hallucinations. \n",
    "    - MIGHT NEED TO FINE-TUNE but the source model doesn't have much parameters that can be used in fine-tuning (maybe there is, and still need to check how - like those from orig Llama2 model were already inherited here in the first place?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = CTransformers(\n",
    "    model=\"TheBloke/Llama-2-7B-Chat-GGML\",\n",
    "    model_file=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "    # model_kwargs = {'temperature':1,'max_new_tokens': 1024, 'top_k' :20}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are an expert journalist. Summarize the given news article using only between 20 to 30 words.\n",
    "<</SYS>>\n",
    "CONTENT:{content}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
    "\n",
    "summarizer_llm = LLMChain(prompt=prompt, llm=llm)\n",
    "summarized_news = summarizer_llm.run(content)\n",
    "print(summarized_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a possible summary of the article in 1 sentence:\\n\\n\"Starting an LLM program has been challenging for me so far, with difficulties in adjusting to academic life.\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test an edge case - less than 20 words\n",
    "summarized_news = summarizer_llm.run(\n",
    "    title=\"Hello World\",\n",
    "    content=\"Hello world I am starting to do LLM now and it's not been a great journey so far\",\n",
    ")\n",
    "summarized_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 3**: Using Gensim\n",
    "- **LIMITATION**: Very slow runtime and just copy-pastes what it thinks are the highlights of the article. Not an advanced model by today's standards in field of NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "summarized_news = summarize(content, word_count=30)\n",
    "print(summarized_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 4**: Using Falcon 7B in Langchain\n",
    "- Link: https://huggingface.co/tiiuae/falcon-7b-instruct\n",
    "- **LIMITATION**: Cannot be imported (for some reason). Even so, after import, many were concerned on it being 10x slower than Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q langchain transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 24 12:03:51 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.42                 Driver Version: 537.42       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8               4W /  60W |    194MiB /  4096MiB |     20%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from langchain import HuggingFacePipeline, LLMChain, PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"  # tiiuae/falcon-40b-instruct\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",  # task\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - made my pc crash\n",
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model_id=\"tiiuae/falcon-7b-instruct\"\n",
    "# tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "# model=AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={\"temperature\": 0.5})\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an expert journalist and a reader that can highlight the most important points written in a news article; \n",
    "    You need to summarize a news article in just 1 sentence similar to a headline but more descriptive. \n",
    "    Add a second sentence only if necessary. Limit the total words between 20 to 30 words and do not use too many commas. Make it sound logical;\n",
    "\n",
    "    TITLE: {title}\n",
    "    CONTENT: {content}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"title\", \"content\"])\n",
    "\n",
    "summarizer_llm = LLMChain(prompt=prompt, llm=llm)\n",
    "summarized_news = summarizer_llm.run(title=title, content=content)\n",
    "print(summarized_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 5**: Using BART from Hugging Face\n",
    "- Link: https://huggingface.co/facebook/bart-large-cnn\n",
    "- **LIMITATION**: Very long inference time and the output is always just a snippet of the original text (very inaccurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer_llm = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "summary = summarizer_llm(content, max_length=130, min_length=30, do_sample=False)\n",
    "summarized_news = summary[0][\"summary_text\"]\n",
    "print(summarized_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 6**: Using Fine-tuned LongT5 transformer from Hugging Face\n",
    "- Link: https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary\n",
    "- The best performing small LLM so far! Next steps for improvement:\n",
    "    - Fine-tune further using cnn_news dataset and other open-source datasets of HF containing predictor = Entire news article and target = the summarized version of article\n",
    "    - Experiment with different hyperparameters of BeamSearch\n",
    "        - Reference: https://huggingface.co/blog/how-to-generate\n",
    "- **LIMITATION**: Cannot understand full context of a news yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 368. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=184)\n",
      "c:\\Users\\Earl\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:859: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhilHealth says it's been attacked by a bunch of ransomware, and they've taken down its entire site and app. Then they shut it down for 24 hours to make sure no one can get into it again.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer_llm = pipeline(\n",
    "    \"summarization\",\n",
    "    \"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n",
    ")\n",
    "\n",
    "summary = summarizer_llm(content)\n",
    "summarized_news = summary[0][\"summary_text\"]\n",
    "print(summarized_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
